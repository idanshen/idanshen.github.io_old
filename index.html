<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <title>Idan Shenfeld</title>
  
  <meta name="author" content="Idan Shenfeld">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Idan Shenfeld</name>
              </p>
              <p>I am a first year Ph.D. student in EECS at <a href="https://csail.mit.edu">MIT CSAIL</a> advised by Professor <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>.  I'm currently interested in making it easier and faster to train RL agents, especially in challenging cases such as history-dependent policies and partial observability.
              </p>
			  <p>
               Before MIT, I worked as an AV applied researcher at <a href="https://news.gm.com/newsroom.detail.html/Pages/news/us/en/2021/oct/1006-ultracruise.html">GM Ultra Cruise project</a>. My main research there was on 3D segmentation and detection algorithms from an array of RGB cameras. In my time there I had the pleasure of working with  <a href="https://scholar.google.com/citations?user=L0cJiRwAAAAJ&hl=en&oi=sra">Dr. Netalee Efrat Sela</a> and <a href="https://scholar.google.com/citations?user=YBQnkjIAAAAJ&hl=fil">Dr. Shaul Oron</a>.
              </p>
              <p>
               Prior to that, I completed my bachelor's degree in EECS from <a href="https://www.technion.ac.il/en/home-2/">the Technion</a> where I worked with Professor <a href="https://avivt.github.io/avivt/">Aviv Tamar</a>. During my bachelor's degree I was supported by the Rothschild Fellowship.
              </p>
              <p style="text-align:center">
                <a href="mailto:idanshen@mit.edu">Email</a> &nbsp/&nbsp
                <a href="data/idan_resume.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/idan-shenfeld/"> LinkedIn </a> &nbsp/&nbsp
				<a href="https://scholar.google.com/citations?user=XYrBgyoAAAAJ&hl=en&oi=sra">Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/IdanShenfeldLarge.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/IdanShenfeldLarge.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>            
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Figure_1_2.png" alt="PontTuset" width="240" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/forum?id=Hk3m8Nh7mn">
                <papertitle>TGRL: An Algorithm for Teacher Guided Reinforcement Learning</papertitle>
              </a>
              <br>
		<strong>Idan Shenfeld</strong>,
		<a href="https://williamd4112.github.io/">Zhang-Wei Hong</a>,
		<a href="https://avivt.github.io/avivt/">Aviv Tamar</a>,
		<a href="http://people.csail.mit.edu/pulkitag/"> Pulkit Agrawal </a>,
              <br>
              <em>ICML</em>, 2023
              <br>
              <em>RRL Workshop at ICLR Spotlight</em> 2023, <strong>spotlight</strong>
              <br>
	      <a href="https://sites.google.com/view/tgrl-paper/home">site</a> /
              <a href="https://openreview.net/forum?id=Hk3m8Nh7mn">openreview</a> /
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:C42ozZoNckwJ:scholar.google.com/&output=citation&scisdr=Cm3GHR8ZEI3K0gEsYSs:AGlGAw8AAAAAZJsqeSvDpTVI_E0FtRhm0immqUg&scisig=AGlGAw8AAAAAZJsqeX480sw7D6L2bRrIoSIWs7s&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>
              <p></p>
              <p> Learning from rewards (i.e., reinforcement learning or RL) and learning to imitate a teacher (i.e., imitation learning) are two established approaches for solving sequential decision-making problems. To combine the benefits of these different forms of learning, it is common to train a policy to maximize a combination of reinforcement and imitation learning objectives. However, without a principled method to balance these objectives, prior work used heuristics and problem-specific hyper-parameter searches to balance the two objectives. We present a \textit{principled} approach, along with an approximate implementation for \textit{dynamically} and \textit{automatically} balancing when to follow the teacher and use rewards. The main idea is to adjust the importance of teacher supervision by comparing the agent's performance to the counterfactual scenario of the agent learning without teacher supervision and only from rewards. If incorporating teacher supervision improves performance, the importance of teacher supervision is increased and otherwise it is decreased. TGRL outperforms strong baselines across diverse domains without hyper-parameter tuning.
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/plan.png" alt="PontTuset" width="240" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2021/hash/248024541dbda1d3fd75fe49d1a4df4d-Abstract.html">
                <papertitle>Offline Meta Reinforcement Learning - Identifiability Challenges and Effective Data Collection Strategies</papertitle>
              </a>
              <br>
			  <a href="https://scholar.google.com/citations?user=baGUoIEAAAAJ&hl=en&oi=sra">Ron Dorfman</a>,
              <strong>Idan Shenfeld</strong>,
              <a href="https://avivt.github.io/avivt/">Aviv Tamar</a>
              <br>
              <em>NeurIPS</em>, 2021
              <br>
              <a href="https://openreview.net/forum?id=IBdEfhLveS">openreview</a> /
              <a href="https://proceedings.neurips.cc/paper/11975-/bibtex">bibtex</a>
              <p></p>
              <p> Consider the following instance of the Offline Meta Reinforcement Learning (OMRL) problem: given the complete training logs of N conventional RL agents, trained on N different tasks, design a meta-agent that can quickly maximize reward in a new, unseen task from the same task distribution. In particular, while each conventional RL agent explored and exploited its own different task, the meta-agent must identify regularities in the data that lead to effective exploration/exploitation in the unseen task. Here, we take a Bayesian RL (BRL) view, and seek to learn a Bayes-optimal policy from the offline data. Building on the recent VariBAD BRL approach, we develop an off-policy BRL method that learns to plan an exploration strategy based on an adaptive neural belief estimate. However, learning to infer such a belief from offline data brings a new identifiability issue we term MDP ambiguity. We characterize the problem, and suggest resolutions via data collection and modification procedures.
              </p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
              <a href="https://github.com/jonbarron/jonbarron_website">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
